\section{Neural networks}
\textcolor{blue}{Explain the use for this section.}\\
\noindent Neural networks are machine learning algorithms inspired by research on the structure and inner workings of brains. \textcolor{red}{[Insert quote (Rosenblatt?)]} Though in the beginning \gls{nn} were not used in computer sciences due to computational limitations \textcolor{red}{[Citation]} they are now a major source of innovation across multiple disciplines. Their capability of pattern recognition and classification has already been successfully applied to a wide range of problems not only in commercial applications but also many scientific fields. \textcolor{red}{[Quote a few scientific usecases here. Of course using the one for gw but also other disciplines.]} Major use cases in the realm of gravitational wave analysis have been classification of glitches in the strain data of \gls{gw}-detectors \textcolor{red}{[Citation]} and classification of strain data containing a \gls{gw} versus pure noise \textcolor{red}{[Citation]}. \textcolor{red}{A few more notable examples include [list of citations].}\\
In this section the basic principles of \gls{nn} will be introduced and notation will be set. The concept of backpropagation will be introduced \textcolor{red}{and extended to a special and for this work important kind of \gls{nn}. (maybe use the term ''convolution'' here already?)} It will be shown that learning in \gls{nn} is simply a mathematical minimization of errors that can largely be understood analytically.\medskip\\
Large portions of this section are inspired and guided by \cite{deep_learning_beginning, deep_learning_book}.

\begin{comment}
Neural networks have become a new and major player in data sciences over the past few years. They have proven to be very good at classification and interpolation. \textcolor{red}{[Insert ref]} Therefore and due to their computational efficiency they seem to be a compelling option even for scientific use cases and have been successfully applied to the classification and basic parameter estimation of \gls{gw}-data.\\
This sections aims to convey the basics of neural networks and the layers that are being utilized in this work. After having read this section it should be clear that neural networks are simply a mathematical model and that there is no magic involved. \textcolor{red}{(Maybe this is too prosa-like and/or should be put into the introduction)}
\end{comment}

\subsection{Neurons, layers and networks}\label{sec:basics_neuron_network}
\textcolor{blue}{What is the general concept of a neural network? How does it work? How does backpropagation work? How can one replicate logic gates? (cite online book)}\\
\noindent The basic building block of a \gls{nn} is - as the name suggests - a \emph{neuron}. This neuron is a function mapping inputs to a single output.\\
In general there are two different kinds of inputs to the neuron. Those that are specific to the neuron itself and those that the neuron receives as an outside stimulus. We write the neuron as
\begin{equation}\label{def:neuron}
n: \R^k\times\R\times\R^k\to\R ;\ \ \ (\vec{w}, b, \vec{x})\mapsto n(\vec{w}, b, \vec{x})\coloneqq a(\vec{w}\cdot\vec{x}+b),
\end{equation}
where $\vec{w}$ are called weights, $b$ is a bias value, $\vec{x}$ is the outside stimulus and $a$ is a function known as the \emph{activation function}\textcolor{red}{ (change this to not be emphasized if it is not used for the first time here)}. The weights and biases are what is tweaked to control the behavior of the neuron, whereas the outside stimulus is not controllable in that sense. A usual depiction of a neuron and its structure is shown in \autoref{fig:neuron}.\\
The activation function is a usually nonlinear scalar function
\begin{equation}\label{def:activation_function}
a:\ \R\to\R
\end{equation}
determining the scale of the output of the neuron. The importance of this activation function and its nonlinearity will be touched upon a little later.\\
To understand the role of each part of the neuron, consider the following activation function:
\begin{equation}\label{def:step_activation}
a(y) = 
\begin{cases}
	1,& y> 0\\
	0,& y\leq 0
\end{cases}.
\end{equation}
With this activation function, the neuron will only send out a signal (or ''fire'') if the input $y$ is greater than 0. Therefore, in order for the neuron to fire, the weighted sum of the inputs $\vec{w}\cdot\vec{x}$ has to be larger than the negative bias $b$. This means, that the weights and biases control the behavior of the neuron and can be optimized to get a specific output.\\
The effects of changing the weights makes individual inputs more or less important. The closer a weight $w_i$ is to zero, the less impact the corresponding input value $x_i$ will have. Choosing a negative weight $w_i$ results in the corresponding input $x_i$ being inverted, i.e. the smaller the value of $x_i$ the more likely the neuron is to activate and vice versa.\\
Changing the bias to a more negative value will result in the neuron having fewer inputs it will fire upon, i.e. the neuron is more difficult to activate. The opposite is true for larger bias values. So increasing it will result in the neuron firing for a larger set of inputs.\\
As an example consider a neuron with activation function \eqref{def:step_activation}, weights $\vec{w}={(w_1, w_2)}^T=(1, 1)$, bias $b=-1.5$ and inputs $(x_1,x_2)\in{\{0,1\}}^2$. Choosing the weights and biases in this way results in the outputs shown in \autoref{tab:and_neuron}. This goes to show, that neurons can replicate the behavior of an ''and''-gate. Other logical gates can be replicated by choosing the weights and biases in a similar fashion (See first section of \autoref{app:Full_adder}).
\begin{table}
\begin{center}
\begin{tabular}{c c|c}
$x_1$ & $x_2$ & $a(\vec{w}\cdot\vec{x}+b)$\\
\hline
$0$ & $0$ & $0$\\
$0$ & $1$ & $0$\\
$1$ & $0$ & $0$\\
$1$ & $1$ & $1$\\
\end{tabular}
\caption{Neuron activation with activation function \eqref{def:step_activation}, weights $\vec{w}={(w_1, w_2)}^T=(1, 1)$, bias $b=-1.5$ and inputs $(x_1,x_2)\in{\{0,1\}}^2$. Choosing the weights and biases in this way replicates an ''and''-gate.}\label{tab:and_neuron}
\end{center}
\end{table}

\begin{figure}
\centering
\input{tikzgraphics/tikz_neuron}
\caption{Depiction of a neuron with inputs $\vec{x}={(x_1, x_2, x_3)}^T$, weights $\vec{w}$, bias $b$ and activation function $a$.}\label{fig:neuron}
\end{figure}

\medskip
\textcolor{blue}{Use the introduction of the and-neuron from above to introduce the concept of networks in a familiar way. Having logic gates enables us to build more complex structures, such as full adders and hence we can, in principle, calculate any function a computer can calculate. Only afterwards introduce layers as a way of structuring and formalizing networks.}\\
\noindent Since all basic logic gates can be replicated by a neuron, it is a straight forward idea to connect them into more complicated structures, like a full-adder (see \autoref{app:Full_adder}). These structures are than called neural networks, as they are a network of neurons. The example of the full-adder demonstrates the principle of a \gls{nns} perfectly. It's premise is to connect multiple simple functions, the neurons, to form a network, that can solve tasks the individual building blocks can't.\\
In other words, a network aims to calculate some general function by connecting multiple easier functions together. This highlights the importance of the activation function, as it introduces nonlinearities into the network. Without these a neural network would not be able to approximate a nonlinear function such as the XOR-Gate used in \autoref{app:Full_adder} (section 6.1 in \cite{deep_learning_book}), which caused the loss of interest in \gls{nns} around 1940 (section 6.6 in \cite{deep_learning_book}).\medskip\\
Since \gls{nns} are the main subject of \autoref{sec:backpropagation} and since it will be a bit more mathematical, some notation and nomenclature is introduced to structure the networks.\\
Specifically each network is composed of multiple layers. Each layer consists of one or multiple neurons and each neuron has inputs only from the previous layer.  Formally we write
\begin{equation}
\mathcal{L}:\R^{k+l}\times\R^l\times\R^k\to\R^l;\ (W, \vec{b}, \vec{x})\mapsto\mathcal{L}(W,\vec{b},\vec{x})\coloneqq
\begin{pmatrix}
n_1\lr{{(W_1)}^T,b_1,\vec{x}}\\
\vdots\\
n_l\lr{{(W_l)}^T,b_l,\vec{x}}
\end{pmatrix},
\end{equation}
where $n_i$ is neuron $i$ on that layer and $W_i$ is the $i$-th row of a $k\times l$-matrix. In principle this definition can be extended to tensors of arbitrary dimensions. This would however only complicate the upcoming sections notationally and the principle should be clear from this minimal case, as dot products, sums and other operations have their according counterparts in tensor calculus. As a further step of formal simplification we will assume that all neurons $n_i$ share the same activation function $a$. This does not limit the ability of networks that can be written down, since if two neurons have different activation functions, they can be viewed as two different layers connected to the same previous layer. Their output will than be merged afterwards (see \autoref{fig:diff_activation_functions_layer}).\\
\begin{figure}
\centering
\input{tikzgraphics/tikz_layer_different_activations}
\caption{Depiction of how a layer ($\mathcal{L}_\text{mid}$) consisting of neurons with different activation functions ($n_1$ and $n_2$) can be split into two separate layers ($\mathcal{L}_{n_1}$ and $\mathcal{L}_{n_2}$).}\label{fig:diff_activation_functions_layer}
\end{figure}

\noindent With this simplification one can write a layer simply as
\begin{equation}
\mathcal{L}(W,\vec{b},\vec{x})=a(W\cdot\vec{x}+\vec{b}),
\end{equation}
where it is understood, that the activation function $a$ acts component wise on the resulting $l$-dimensional vector.\\
In this fashion a network consisting of a chain of layers $\mathcal{L}_\text{in}$, $\mathcal{L}_\text{mid}$, $\mathcal{L}_\text{out}$ can be written as
\begin{align}
\mathcal{N} & \lr{W^\text{in}, \vec{b}^\text{in}, W^\text{mid}, \vec{b}^\text{mid}, W^\text{out}, \vec{b}^\text{out}, \vec{x}}\nonumber\\
\phantom{\mathcal{N}} & \coloneqq\mathcal{L}_\text{out}\lr{W^\text{out}, \vec{b}^\text{out}, \mathcal{L}_\text{mid}\lr{W^\text{mid}, \vec{b}^\text{mid}, \mathcal{L}_\text{in}\lr{W^\text{in}, \vec{b}^\text{in}, \vec{x}}}}\nonumber\\
\phantom{\mathcal{N}} & = a_\text{out}\lr{\vec{b}^\text{out}+W^\text{out}\cdot a^\text{mid}\lr{\vec{b}^\text{mid}+W_\text{mid}\cdot a_\text{in}\lr{\vec{b}^\text{in}+W_\text{in}\cdot \vec{x}}}}.
\end{align}
Hence a network can be understood as a set of nested functions.\\
An important point with the definitions above is that the layers get their input only from their preceding layers. Especially no loops are allowed, i.e. getting input from some subsequent layer is not permitted. A network of the first kind is called a \emph{feed forward neural network} (\gls{ffn}), as for one input each layer gets invoked only once. There are also other architectures called \emph{recurrent neural networks} (\gls{rnn}), which also allow for loops in the networks and work by propagating the activations in discrete time steps. These kinds of networks are in principle closer to the inner workings of the human brain, but in practice show worst performance and are therefore not used or discussed further in this work. \textcolor{red}{[Citations], maybe also mention that RNNs have shown good performance in time series data (which we are working with) but other studies (paper Frank sent around) have shown that TCN also do the job}\\
A \gls{ffn} in general consists of three different parts called the input-, output- and hidden layer/layers. The role of the input- and output-layers is self explanatory; they are the layers where data is fed into the network or where data is read out. Therefore their shape is determined by the data the network is being fed and the expected return. The hidden-layers on the contrary are called ''hidden'', as their shape and size is not defined by the data. Furthermore the hidden layers do not see the input or labels directly, which means, that the network itself has to ''decide'' on how to use them (page 165 in \cite{deep_learning_book}). \autoref{fig:hidden_layer} shows an example of a simple network with a single hidden layer. In principle there could be any number of hidden layers with different sizes. In this example the input is n-dimensional and the output 2-dimensional. If the input was changed to be (n-1)-dimensional, the same hidden-layer could be used, as its size does not depend on the data or output. Therefore when designing a network architecture, one designs the shape and functionality of the hidden layers. How well it performs is mainly governed by theses layers. \textcolor{red}{[Can I find citation for this last statement?]}\\
A \gls{nn} is called \emph{deep}, if it has multiple hidden layers. \textcolor{red}{[Citation]}
\begin{figure}
\centering
\input{tikzgraphics/tikz_hidden_layer}
\caption{A depiction of a simple network with a single input-, hidden- and output-layer. The input-data is a n-dimensional vector ${(x_1, \dotsc, x_n)}^T$ and the output is a 2-dimensional vector. In this picture it looks like the hidden layer has the same number of neurons as the input layer. This does not necessarily have to be the case. Lines between two neurons indicate, that the output of the left neuron serves as weighted input for the right one.}\label{fig:hidden_layer}
\end{figure}

\subsection{Backpropagation}\label{sec:backpropagation}
\textcolor{red}{The beginning of this section feels very wordy and repetitive. Break it down!}\\
In \autoref{sec:basics_neuron_network} the basics of a \gls{nn} where discussed and the example of a network replicating a binary full-adder (see \autoref{app:Full_adder}) showed the potential of these networks, when the weights and biases are chosen correctly. The example actually proofs that a sufficiently complicated network can - in principle - calculate any function a computer can, as a computer is just a combination of logic gates, especially binary full-adders.\footnote{There is an even stronger statement called the universal approximation theorem, which states that any Borel measurable function on a finite-dimensional space can be approximated to any degree with a \gls{nn} with at least one single hidden layer of sufficient size. (p. 194 \cite{deep_learning_book})}\cite{deep_learning_beginning}\\
The question therefore is how to choose the weights in a network for it to approximate some function optimally. For the binary full-adder the weights and biases were chosen by hand, as the problem the network was trying to solve was rather simple. A more general approach however would be beneficial, as not all problems are this simple. Therefore the goal is to design some network and let it learn/optimize the weights and biases such that the error between the actual function and the estimate of the network is minimal.\\
To do this, some known and labeled data is necessary, in order for the network being able to compare its output to some ground truth and adjust its weights and biases to minimize some error function. This way of optimizing the weights and biases is called \emph{training}. The data used during training is hence called training data or training set. To be a bit more specific, the analyzed data in this work is some time series. The output of this analysis will be some scalar number; the \gls{snr}. Therefore the network receives some data as input, of which the true \gls{snr}-value is known. This true value will be called \emph{label} from here on out. The network will produce some value from this input data and compare it to what \gls{snr} was provided as label. From there it will try to optimize the weights and biases to best fit the function that maps $\text{data}\to\text{\gls{snr}}$. This process of optimizing the weights and biases in the way described below is enabled by a process called backpropagation, as the error propagates from the last to the first layer. The meaning of this will become clearer in the upcoming paragraphs.\\
So far only the abstract term ''error'' was used. This error, in machine learning language, is called the \emph{loss function} and in general is defined by
\begin{equation}\label{def:general_loss}
L:\R^{l\times k}\times\R^{l\times k}\to\R;\ (y_\text{net}, y_\text{label})\mapsto L(y_\text{net}, y_\text{label}),
\end{equation}
where $l$ is the number of training samples used to estimate the error and $k$ is the dimension of the network output.\\
When doing a regressive fit, one of the standard error functions is the \emph{mean squared error} (\gls{mse}), which is the loss function mainly used in this work and that is defined by
\begin{equation}\label{def:loss_mean_squared_error}
L:\R^{l\times k}\times\R^{l\times k}\to\R;\ (y_\text{net}, y_\text{label})\mapsto L(y_\text{net}, y_\text{label})\coloneqq\frac{1}{l}\sum_{i=1}^l{(\vec{y}_{\text{net},i}-\vec{y}_{\text{label},i})}^2.
\end{equation}
A more thorough discussion and justification for using \gls{mse} as loss can be found in section 5.5 and 6.2.1.1 of \cite{deep_learning_book}.\\
To minimize this loss, the weights and biases of the different layers are changed, usually using an algorithm called \emph{gradient decent}. It works by calculating the gradient of some layer with respect to its weights and biases and taking a step in the opposite direction. For notational simplicity we'll denote the weights and biases of a network by $\theta$ and call them collectively parameters. It is understood that $\theta=\lr{W^{1},b^1, W^2, b^2, \cdots}$. Gradient decent is than given by
\begin{equation}\label{def:gradient_decent}
\theta '=\theta - \epsilon\ \nabla_\theta L\lr{y_\text{net}(\theta), y_\text{label}},
\end{equation}
where $\epsilon$ is called learning rate and controls how large of a step is taken on each iteration.\\
This formula assumes, that all samples from the training set are used to calculate the gradient. In practice this would be too computationally costly. Therefore the training set is split into multiple parts, called mini-batches. A step of the gradient decent is than made using only the samples from one mini-batch. This alteration of gradient decent goes by the name of \emph{stochastic gradient decent}. The larger the mini-batch, the more accurate the estimate of the gradient and therefore fewer steps are needed to get to lower values of the loss. Each step however takes longer to calculate. This means one has to balance the benefits and drawbacks of the mini-batch size.\medskip\\
The real work of training a network now lies in calculating the gradient $\nabla_\theta L\lr{y_\text{net}(\theta), y_\text{label}}$, which is a challenge, as $\theta$ usually consists of at least a few hundred thousand weights and biases. The algorithm, that is used to calculate this gradient, is called backpropagation or simply backprop and is mostly a iterative application of the chain rule.\\
For simplicity assume we have a network $\mathcal{N}\lr{\theta, \vec{x}}$ consisting of $n$ consecutive layers $\mathcal{L}^1,\cdots,\mathcal{L}^n$ with weights $W^1,\cdots,W^n$, biases $\vec{b}^1,\cdots,\vec{b}^n$ and activation functions $a_1,\cdots, a_n$. The network will be trained by minimizing the loss given in \eqref{def:loss_mean_squared_error}. Calculating the gradient $\nabla_\theta L\lr{y_\text{net}(\theta), y_\text{label}}$ requires to calculate $\nabla_{W^1}L, \cdots,\nabla_{W^n}L$ and $\nabla_{\vec{b}^1},\cdots,\nabla_{\vec{b}^n}$, where
\begin{equation}\label{def:matrix_gradient}
\nabla_{W^i}L\coloneqq
\begin{pmatrix}
	\partial_{W^i_{11}}L & \cdots & \partial_{W^i_{1l}}L\\
	\vdots & \ddots & \vdots\\
	\partial_{W^i_{k1}}L & \cdots & \partial_{W^i_{kl}}L
\end{pmatrix},
\end{equation}
for $W^i\in \R^{k\times l}$ and
\begin{equation}\label{def:vector_gradient}
\nabla_{\vec{b}^i}L\coloneqq
\begin{pmatrix}
	\partial_{b^i_1}L\\
	\vdots\\
	\partial_{b^i_k}L
\end{pmatrix},
\end{equation}
for $\vec{b}^i\in\R^k$.\\
To calculate $\partial_{W^i_{jk}}L$ and $\partial_{b^i_j}L$, define
\begin{align}\label{def:backprop_z}
z^n & \coloneqq\vec{b}^n+W^n\cdot a_{n-1}\lr{z^{n-1}}\nonumber\\
z^1 & \coloneqq\vec{b}^1+W^1\cdot\vec{x},
\end{align}
such that
\begin{equation}
\mathcal{N}\lr{\theta,\vec{x}}=a_n\lr{z_n}.
\end{equation}
To save another index, we will assume a mini-batch size of 1. For a larger mini-batch size one simply has to average over the individual gradients, as sums and derivatives commute.\\
With this in mind, the loss is given by
\begin{equation}
L\lr{y_\text{net}, y_\text{label}}=L\lr{\mathcal{N}\lr{\theta,\vec{x}}, y_\text{label}}=L\lr{a_n\lr{z_n}, y_\text{label}}=\lr{a_n\lr{z_n}- \vec{y}_\text{label}}^2.
\end{equation}
To start off derive this loss by some scalar that only $z^n$ depends on.
\begin{equation}
\partial_{\theta_j}\lr{a_n\lr{z_n}- \vec{y}_\text{label}}^2=\lr{\partial_{\theta_j}a_n\lr{z_n}}\lr{2\lr{a_n\lr{z_n}- \vec{y}_\text{label}}}
\end{equation}
From there calculate $\partial_{\theta_j}a_n\lr{z_n}$, remembering, that $a_n$ and $z_n$ are both vectors.
\begin{align}\label{def:start_backprop}
\partial_{\theta_j}a_n\lr{z_n} & = \partial_{\theta_j}\sum_i a_n^i\lr{z_{n,1}\lr{\theta_j},\dotsc, z_{n,k}\lr{\theta_j}} \vec{e}_i\nonumber\\
& = \sum_i \sum_{m=1}^k \lr{\partial_{\theta_j}z_{n, m}\lr{\theta_j}}\lr{\partial_{z_{n, m}} a_n^i\lr{z_{n,1}\lr{\theta_j},\dotsc, z_{n,k}\lr{\theta_j}}} \vec{e}_i\nonumber\\
& = \sum_i \lr{\lr{\partial_{\theta_j}z_n}\cdot \lr{\nabla_{z_n} a_n^i}}\vec{e}_i
\end{align}
Since all activation functions $a_n^i$ on a layer are the same, the gradient $\lr{\nabla_{z_n} a_n^i}$ simplifies to $\partial_z \restr{a(z)}{z=z_{n,i}}$. With this one gets
\begin{equation}\label{def:start_backprop_simple}
\partial_{\theta_j}a_n\lr{z_n} = \lr{\partial_{\theta_j} z_n} \odot \partial_z \restr{a_n(z)}{z=z_n},
\end{equation}
where $\odot$ denotes the Hadamard product.
The final step to understanding backpropagation is to evaluate $\partial_{\theta_j}z_n$. For now assume that $\theta_j$ is some weight on a layer that is not the last layer.
\begin{align}\label{def:backprop_recurs_1}
\partial_{\theta_j}z_n & = \partial_{\theta_j}\lr{\vec{b}^n+W^n\cdot a_{n-1}\lr{z_{n-1}}}\nonumber\\
& = \partial_{\theta_j}\lr{W^n\cdot a_{n-1}\lr{z_{n-1}}}\nonumber\\
& = W^n\cdot\partial_{\theta_j}a_{n-1}\lr{z_{n-1}}
\end{align}
Inserting \eqref{def:backprop_recurs_1} into \eqref{def:start_backprop_simple} yields the recursive relation
\begin{equation}\label{def:backprop_recursive_relation}
\partial_{\theta_j}a_n\lr{z_n} = \lr{W^n\cdot\partial_{\theta_j}a_{n-1}\lr{z_{n-1}}}\odot\partial_z \restr{a_n(z)}{z=z_n}.
\end{equation}
The recursion stops, when it reaches the layer the weight $\theta_j$ is located on and evaluates to (assuming $\theta_j$ is part of layer $k$)
\begin{equation}\label{def:recursion_stop_weight}
\partial_{\theta_j}a_k\lr{z_k}=\lr{\partial_{\theta_j}W^k}\cdot a_{k-1}\lr{z_{k-1}}.
\end{equation}
The derivative can also be expressed in an analytical form, by utilizing, that the Hadamard product is commutative and can be expressed in terms of matrix multiplications. To do so define
\begin{equation}
\left[\Sigma\lr{\vec{x}}\right]_{ij}=
\begin{cases}
	x_i,& i = j\\
	0,& \text{otherwise}
\end{cases}.
\end{equation}
With this definition equation \eqref{def:backprop_recursive_relation} can be written as
\begin{equation}
\partial_{\theta_j}a_n\lr{z_n} = \Sigma\lr{\partial_z \restr{a_n(z)}{z=z_n}}W^n\cdot\partial_{\theta_j}a_{n-1}\lr{z_{n-1}}
\end{equation}
and the recursion can be solved to yield
\begin{equation}
\partial_{\theta_j}a_n\lr{z_n} = \left[\prod_{l=0}^{n-k+1}\Sigma\lr{\restr{\partial_z a_{n-l}\lr{z}}{z=z_{n-l}}}\cdot W^{n-l}\right]\cdot \Sigma\lr{\restr{\partial_z a_k\lr{z}}{z=z_k}}\cdot\lr{\partial_{\theta_j}W^k}a_{k-1}\lr{z_{k-1}}.
\end{equation}
\textcolor{red}{If there is time, check the equations below, as I did not thoroughly recompute them.}\\
The same computation can be done if $\theta_j$ is a bias instead of a weight. When this computation is done, equation \eqref{def:backprop_recursive_relation} still holds, but the stopping condition \eqref{def:recursion_stop_weight} is simplified to
\begin{equation}
\partial_{\theta_j}a_k\lr{z_k}=\partial_{\theta_j}\vec{b}^k.
\end{equation}
From this the analytic form can be computed to be
\begin{equation}
\partial_{\theta_j}a_n\lr{z_n} = \left[\prod_{l=0}^{n-k+1}\Sigma\lr{\restr{\partial_z a_{n-l}\lr{z}}{z=z_{n-l}}}\cdot W^{n-l}\right]\cdot \Sigma\lr{\restr{\partial_z a_k\lr{z}}{z=z_k}}\cdot\partial_{\theta_j}\vec{b}^k.
\end{equation}
Using the recursive formula \eqref{def:backprop_recursive_relation} now justifies the term ''backpropagation''. When a sample is evaluated, it is passed from layer to layer starting at the front. Therefore this is called a \emph{forward pass}. The output the network gives for a single forward pass will probably differ from the label and hence has an error (quantified by the loss function). This error is used to calculate the gradient and adjusts the parameters of the network. The way this is done is given by \eqref{def:backprop_recursive_relation}. It starts at the last layer and propagates back through the network until it reaches the layer of the weight that should be adjusted.\\
With these formulae one could in principle calculate the gradient of the loss with respect to all parameters $\theta$ and use this gradient to optimize and train a network. In reality this would still be too slow and computationally costly. Instead each layer (or rather each operation) has a backpropagation method associated to it, that returns the gradient based on a derivative to one of its inputs and the gradient from the previous layer.\\
For clarification, consider an operation that multiplies two matrices $A$ and $B$ and say the gradient calculated by the backpropagation method of the previous layer returned $G$ as its gradient. The backpropagation method for the matrix multiplication now needs to implement the derivative with respect to $A$ and the derivative with respect to $B$. Thus it will return $G\cdot B^T$ when derived by $A$ and $G\cdot A^T$ when derived by $B$. (section 6.5.6 \cite{deep_learning_book})\\
The full backpropagation algorithm than only has to call the backpropagation methods of each layer/operation. (For a more thorough discussion see section 6.5 of \cite{deep_learning_book}.)

\subsection{Training and terminology}
In the previous \autoref{sec:backpropagation} the backpropagation algorithm was introduced as the method used for the network to learn. It used some labeled data to compare its output to and adjust the parameters accordingly. This labeled data was called the training set. In principle the network could be trained over and over again on the same data to further improve the performance of the network. The worst one could fear for is a gradient that vanishes, as the global or a local minimum in the loss is reached.\\
In practice this is true only partially. At some point the network will start ''memorizing'' the samples it has seen in the training set. When a network shows this behavior during training it is called \emph{overfitting}. This is a problem not only known in machine learning but also with regressive fits and has the same reason; too many free parameters. Consider a parabola sampled at $n$ points. If a regressive fit is done, the best choice for a basis would be a parabola $f(x)=a x^2+b x + c$ with the three free parameters $a$, $b$ and $c$. If $n\geq 3$ a regressive fit minimizing the \gls{mse} would recover the original parabola that was sampled by the $n$ points. However one could also use a polynomial of degree $m>n$ to find a function that runs exactly through all $n$ points and thus minimizes the \gls{mse} to the same value of zero too. (see \autoref{fig:overfitting})\\
There are however two differences between the two cases. The most obvious one is the number of free parameters. The parabola has three parameters, whereas the polynomial of degree $m$ has $m$ free parameters. As $m>n$ was required, there is at least one parameter that cannot be fixed by the data and is therefore still free. The second difference is the behavior of the \gls{mse} when another point is added to the set but the regression is not redone. For the parabola the \gls{mse} will stay zero, for the polynomial of degree $m$ however the \gls{mse} will most likely be greater than zero, as the true parabola isn't matched. (Compare lower right of \autoref{fig:overfitting}) The first difference explains why overfitting takes place, there are too many parameters that can be varied, the second difference gives a way to detect when overfitting takes place. If the \gls{mse} rises on samples that were not used for the regression, overfitting takes place.\\
The same concept can than be applied to \gls{nns}; if the loss of a network is bigger on different data than that used during training, the network is said to overfit. This second set of samples is called the validation set, as it validates the training. Obviously the data in the validation set must stem from the same underlying procedure, that generated the training set. In the context of this work this means, that the waveforms of the training and validation set must share the same parameter-space.\\
Contrary to overfitting, there is also a phenomenon called \emph{underfitting}. This occurs, when the number of free, i.e. trainable, parameters of a network is too low. It manifests usually in an occasionally lower loss value of the validation set when compared to the training set. To overcome this issue one can simply increase the number of trainable parameters the network has. Increasing the number of trainable parameters is also called increasing the \emph{capacity} of the network.\\
Though underfitting is possible, overfitting is usually a lot more common. There are multiple ways to deal with a network, that overfits during training. The first one would be to reduce the capacity of the network. If that is not possible or worsens the results, the second easiest way is to increase the number of samples in the training set. In the realm of this work, this is a possibility, as we use simulated data, that can be generated on demand. For a lot of other applications however this is not feasible and other means are necessary. One way is to use a technique called regularization, which is explained in \autoref{sec:regularization} and applied to our networks as well. Another one, which will not be discussed here, is data augmentation. (See section 7.4 of \cite{deep_learning_book})\\
To tune out the generalization error, which is the difference between training set and validation set loss \textcolor{red}{(Is this true?)}, one adjusts the architecture. If there are multiple different architectures, the best one is chosen by the performance on the validation set. In this way, the validation set is also used to fit the model, as in the end the human who trains the models selects the best performing network. Therefore all given results, that did not occur during training\footnote{An example of a result that comes from training the network would be the loss history.}, come from a third independent set. This set is than called the test set.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{overfitting.png}
\caption{Depiction of overfitting in the classical regression. On the left the originally sampled function $f(x)\coloneqq 3x^2 + 2x + 1$ is shown in dashed and black. The red dots are the samples that are being used for the regression on the right. The top right shows the regression, where $g(x)=a x^2 + b x + c$ was used as a basis and recovered the correct points $a=3$, $b=2$ and $c=1$. All free parameters are fixed by the data. The lower right plot shows a case of overfitting. The same points are now used to fit the parameters $a$, $b$, $c$ and $d$ of the function $h(x)=a x^3 + b x^2 + c x + d$. The analytic solution returns $b=3$, $c=2-a$ and $d=1$ with $a$ being free. Therefore a possible regression could use $a=5$, which is used in the lower right plot. The points used for regression are all hit, hence the \gls{mse} is zero. However if another point on the black dashed line would be used, the fitted model would be off.}\label{fig:overfitting}
\end{figure}

\subsection{Convolution neural networks}
In the previous sections only fully connected layers were used to build networks. These are layers, where each neuron is connected to every neuron on the previous layer. These fully connected layers are called \emph{dense} layers. (See \autoref{fig:dense_layer}) In this section a different kind of layer and variants of it will be motivated and introduced. It is the main driving force of modern neural networks and is called convolution layer.
\subsubsection{Convolution layer}\label{sec:convolution_layer}
\textcolor{blue}{What are the advantages of convolution layers and why do we use them? Disadvantages?}
\begin{figure}
\centering
\input{tikzgraphics/tikz_dense_layer}
\caption{\textcolor{red}{Insert description! Maybe inprove this graphic.}}\label{fig:dense_layer}
\end{figure}
\subsubsection{Max Pooling layer}\label{sec:max_pooling_layer}
\textcolor{blue}{Explain what max pooling does and why it is useful, even when it is counter intuitive.}
\subsubsection{Inception module}\label{sec:inception_module}
\textcolor{blue}{Explain what it is, how it works. (cite google paper) ONLY IF IT IS REALLY USED IN THE FINAL ARCHITECTURE!}\\
Networks consisting of stacked convolution layers as introduced in \autoref{sec:convolution_layer} have had great success in image classification. \cite{deep_learning_book, alex_net, ILSVRC15} As the field of computer vision is one of the most prominent in machine learning and shows great advances, we use networks successfully applied there as a guideline for new architectures. \textcolor{red}{(Rephrase this part maybe, as the section is structured differently now.)}\\
The advantages of convolution layers over classical dense layers are manifold and discussed in further detail in \autoref{sec:convolution_layer}. One of the key advantages however is the comparatively low number of trainable parameters, as the connections are a lot more sparse. The number of these trainable parameters however is still quite large and limits the depth of a Deep-\gls{cnn}. This becomes especially obvious, when one scales the number of filters used in the convolution layers, throughout the network. If the number of filters of two consecutive convolution layers is scaled by a factor $c$, the number of trainable parameters increases by a factor of $c^2$. (scaling the number of filters is one way to increase the capacity of a network and reduce underfitting) \cite{inception_module} Another way to achieve the same effect is to scale the convolution-kernel size. \textcolor{red}{Need to talk about over- and underfitting, model capacity and training data differences in a section before this one! (Probably best after the backprop section)} Larger kernels furthermore provide the capabiltiy to detect larger features within a certain part of the image, but might have the problem of being close to zero, which in turn wastes a lot of computational resources. In this situation an approach that utilizes sparse matrices or tensors would be quite beneficial, if the computational infrastructure supports it efficiently. The advantage gained by the lowered number of computations is however mostly outweight by the computational overhead created. Therefore sparse matrix operations are not feasible at the moment. \cite{inception_module}\\
A workaround for this problem is grouping multiple sparse operations together into matrices that are mostly sparse but contain dense submatrices. The matrix-operations can than be performed efficiently on the sparse matrices, by utilizing the efficient dense operations on the dense submatrices. This is the approach, that the inception modules try to take. They build a single module, that can be viewed as a layer from the outside. It contains multiple small convolution layers, that build up a larger, sparse filter. Using this new architecture, the GoogLeNet won the 2014 \gls{ilsvrc}\footnote{The \gls{ilsvrc} is a yearly competition for computer vision algorithms. It is widely used as a benchmark to judge how well a network (or any other computer vision software) does. It is always the same set of images, where each image belongs to one of about 1000 classes. The top 5 error rate is the relative number of times, the algorithm in use did not return the correct category within its top 5 choices.} image recognition competition in the category ''image classification and localization'', setting a new record for the top 5 error rate, thus proving the effectiveness of the new module. \cite{inception_module, ILSVRC15}\\
As the original work was used to handle 2 dimensional images and thus used 2D-convolutions, the module had to be slightly adjusted to fit the 1 dimensional requirements of the time series data in this work. This was a simple task however, as the difference between the two is simply the shape of the kernel and the way it is moved across the data. With Keras, there are predefined functions to handle 1D and 2D convolutions. The downside of converting the 2 dimensional inception module to a 1 dimensional one however is, that many of the incremental improvements to the module are not applicable, as they rely heavily on the 2D-structure. \cite{inception_v2_v3, inception_v4}\medskip\\
The following paragraphs will describe the module used in this work in greater detail.\\
The module consists of 4 independent towers, each consisting of different layers. The full module is depicted in \autoref{fig:inception_module}.\\
The module consists of three parallel convolution layers, i.e. each of the three layers share the same input. The difference between them is the kernel size. The convolution layers with a larger kernel a preceded by a convolution layer with $16$ filters and a kernel size of $1$. The purpose of this step is to reduce the number of channels used and is called dimensional reduction. This leads to a fixed input size for the larger kernels, regardless of the depth of the input. 
In the original architecture filters of size $1\times1$, $3\times3$ and $5\times5$ were used. Translating them directly to 1 dimensional equivalents, the module should use kernel sizes of $1$, $3$ and $5$. However we empirically found, that the smallest kernel sizes $1$, $2$ and $3$ performed best. \textcolor{red}{(Did I ever try 1,3,5? If not do so!)}\\
Finally a pooling layer as introduced in \autoref{sec:max_pooling_layer} is added as a fourth path. The reasoning behind this step is, that pooling layers have shown great improvements in traditional \gls{cnns} and thus the network should be provided with the option to choose this route as well. For this layer the dimensional reduction takes place only after the pooling procedure.\\
The output of each of these paths is than concatenated along the last axis of the tensor, i.e. along the different channels. For this reason all input to each of the layers is padded with zeros in such a way, that the shape (except for the channels) does not change.
\begin{figure}
\centering
\input{tikzgraphics/tikz_inception_module}
\caption{Shown are the contents and connections of the inception module as used in this work. \textcolor{red}{(If the filter numbers and values change for the final architecture, change them here too.)} The layer Conv1D$\lr{x,y}$ is a 1 dimensional convolution layer with $x$ filters and a kernel size of $y$. Most of the convolution layers with a kernel size of $1$ are used for dimensional reduction. The only exception is the leftmost one, that consists of $96$ filters. The different filter sizes correspond to the ability of detecting features at different scales. The pooling layer is a 1 dimensional pooling layer, that only passes on the maximum value in a bin of size 4. The final layer concatenates the channels of the different towers. This also means, that each tower needs to have the same output-shape, excluding the channels. For this reason all inputs are automatically padded with zeros in such a way, that the output-shapes are correct.}\label{fig:inception_module}
\end{figure}
\subsubsection{Temporal Convolutional Networks}
\textcolor{blue}{Explain what they are, what their advantages are and list works that utilized them.}

\subsection{Regularization}\label{sec:regularization}
\subsubsection{Batch Normalization layer}
\textcolor{blue}{Explain how batch normalization works and why it is useful. (cite according paper)}
\subsubsection{Dropout layer}
\textcolor{blue}{Explain what a dropout layer is, what it does, why it is useful.}
