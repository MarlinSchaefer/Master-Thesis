\section{Neural networks}
\textcolor{blue}{Explain the use for this section.}\\
\noindent Neural networks are machine learning algorithms inspired by research on the structure and inner workings of brains. \textcolor{red}{[Insert quote (Rosenblatt?)]} Though in the beginning \gls{nn} were not used in computer sciences due to computational limitations \textcolor{red}{[Citation]} they are now a major source of innovation across multiple disciplines. Their capability of pattern recognition and classification has already been successfully applied to a wide range of problems not only in commercial applications but also many scientific fields. \textcolor{red}{[Quote a few scientific usecases here. Of course using the one for gw but also other disciplines.]} Major use cases in the realm of gravitational wave analysis have been classification of glitches in the strain data of \gls{gw}-detectors \textcolor{red}{[Citation]} and classification of strain data containing a \gls{gw} versus pure noise \textcolor{red}{[Citation]}. \textcolor{red}{A few more notable examples include [list of citations].}\\
In this section the basic principles of \gls{nn} will be introduced and notation will be set. The concept of backpropagation will be introduced \textcolor{red}{and extended to a special and for this work important kind of \gls{nn}. (maybe use the term ''convolution'' here already?)} It will be shown that learning in \gls{nn} is simply a mathematical minimization of errors that can largely be understood analytically.

\begin{comment}
Neural networks have become a new and major player in data sciences over the past few years. They have proven to be very good at classification and interpolation. \textcolor{red}{[Insert ref]} Therefore and due to their computational efficiency they seem to be a compelling option even for scientific use cases and have been successfully applied to the classification and basic parameter estimation of \gls{gw}-data.\\
This sections aims to convey the basics of neural networks and the layers that are being utilized in this work. After having read this section it should be clear that neural networks are simply a mathematical model and that there is no magic involved. \textcolor{red}{(Maybe this is too prosa-like and/or should be put into the introduction)}
\end{comment}

\subsection{Neurons, layers and networks}\label{sec:basics_neuron_network}
\textcolor{blue}{What is the general concept of a neural network? How does it work? How does backpropagation work? How can one replicate logic gates? (cite online book)}\\
\noindent The basic building block of a \gls{nn} is - as the name suggests - a \emph{neuron}. This neuron is a function mapping inputs to a single output.\\
In general there are two different kinds of inputs to the neuron. Those that are specific to the neuron itself and those that the neuron receives as an outside stimulus. We write the neuron as
\begin{equation}\label{def:neuron}
n: \R^k\times\R\times\R^k\to\R ;\ \ \ (\vec{w}, b, \vec{x})\mapsto n(\vec{w}, b, \vec{x})\coloneqq a(\vec{w}\cdot\vec{x}+b),
\end{equation}
where $\vec{w}$ are called weights, $b$ is a bias value, $\vec{x}$ is the outside stimulus and $a$ is a function known as the \emph{activation function}\textcolor{red}{ (change this to not be emphasized if it is not used for the first time here)}. The weights and biases are what is tweaked to control the behavior of the neuron, whereas the outside stimulus is not controllable in that sense. A usual depiction of a neuron and its structure is shown in \autoref{fig:neuron}.\\
The activation function is a usually nonlinear scalar function
\begin{equation}\label{def:activation_function}
a:\ \R\to\R
\end{equation}
determining the scale of the output of the neuron. The importance of this activation function and its nonlinearity will be touched upon a little later.\\
To understand the role of each part of the neuron, consider the following activation function:
\begin{equation}\label{def:step_activation}
a(y) = 
\begin{cases}
	1,& y> 0\\
	0,& y\leq 0
\end{cases}.
\end{equation}
With this activation function, the neuron will only send out a signal (or ''fire'') if the input $y$ is greater than 0. Therefore, in order for the neuron to fire, the weighted sum of the inputs $\vec{w}\cdot\vec{x}$ has to be larger than the negative bias $b$. This means, that the weights and biases control the behavior of the neuron and can be optimized to get a specific output.\\
The effects of changing the weights makes individual inputs more or less important. The closer a weight $w_i$ is to zero, the less impact the corresponding input value $x_i$ will have. Choosing a negative weight $w_i$ results in the corresponding input $x_i$ being inverted, i.e. the smaller the value of $x_i$ the more likely the neuron is to activate and vice versa.\\
Changing the bias to a more negative value will result in the neuron having fewer inputs it will fire upon, i.e. the neuron is more difficult to activate. The opposite is true for larger bias values. So increasing it will result in the neuron firing for a larger set of inputs.\\
As an example consider a neuron with activation function \eqref{def:step_activation}, weights $\vec{w}={(w_1, w_2)}^T=(1, 1)$, bias $b=-1.5$ and inputs $(x_1,x_2)\in{\{0,1\}}^2$. Choosing the weights and biases in this way results in the outputs shown in \autoref{tab:and_neuron}. This goes to show, that neurons can replicate the behavior of an ''and''-gate. Other logical gates can be replicated by choosing the weights and biases in a similar fashion (See first section of \autoref{app:Full_adder}).
\begin{table}
\begin{center}
\begin{tabular}{c c|c}
$x_1$ & $x_2$ & $a(\vec{w}\cdot\vec{x}+b)$\\
\hline
$0$ & $0$ & $0$\\
$0$ & $1$ & $0$\\
$1$ & $0$ & $0$\\
$1$ & $1$ & $1$\\
\end{tabular}
\caption{Neuron activation with activation function \eqref{def:step_activation}, weights $\vec{w}={(w_1, w_2)}^T=(1, 1)$, bias $b=-1.5$ and inputs $(x_1,x_2)\in{\{0,1\}}^2$. Choosing the weights and biases in this way replicates an ''and''-gate.}\label{tab:and_neuron}
\end{center}
\end{table}

\begin{figure}
\centering
\input{tikzgraphics/tikz_neuron}
\caption{Depiction of a neuron with inputs $\vec{x}={(x_1, x_2, x_3)}^T$, weights $\vec{w}$, bias $b$ and activation function $a$.}\label{fig:neuron}
\end{figure}

\medskip
\textcolor{blue}{Use the introduction of the and-neuron from above to introduce the concept of networks in a familiar way. Having logic gates enables us to build more complex structures, such as full adders and hence we can, in principle, calculate any function a computer can calculate. Only afterwards introduce layers as a way of structuring and formalizing networks.}\\
\noindent Since all basic logic gates can be replicated by a neuron, it is a straight forward idea to connect them into more complicated structures, like a full-adder (see \autoref{app:Full_adder}). These structures are than called neural networks, as they are a network of neurons. The example of the full-adder demonstrates the principle of a \gls{nns} perfectly. It's premise is to connect multiple simple functions, the neurons, to form a network, that can solve tasks the individual building blocks can't.\\
In other words, a network aims to calculate some general function by connecting multiple easier functions together. This highlights the importance of the activation function, as it introduces nonlinearities into the network. Without these a neural network would not be able to approximate a nonlinear function such as the XOR-Gate used in \autoref{app:Full_adder} (section 6.1 in \cite{deep_learning_book}), which caused the loss of interest in \gls{nns} around 1940 (section 6.6 in \cite{deep_learning_book}).\medskip\\
Since \gls{nns} are the main subject of \autoref{sec:backpropagation} and since it will be a bit more mathematical, some notation and nomenclature is introduced to structure the networks.\\
Specifically each network is composed of multiple layers. Each layer consists of one or multiple neurons and each neuron has inputs only from the previous layer.  Formally we write
\begin{equation}
\mathcal{L}:\R^{k+l}\times\R^l\times\R^k\to\R^l;\ (W, \vec{b}, \vec{x})\mapsto\mathcal{L}(W,\vec{b},\vec{x})\coloneqq
\begin{pmatrix}
n_1\lr{{(W_1)}^T,b_1,\vec{x}}\\
\vdots\\
n_l\lr{{(W_l)}^T,b_l,\vec{x}}
\end{pmatrix},
\end{equation}
where $n_i$ is neuron $i$ on that layer and $W_i$ is the $i$-th row of a $k\times l$-matrix. In principle this definition can be extended to tensors of arbitrary dimensions. This would however only complicate the upcoming sections notationally and the principle should be clear from this minimal case, as dot products, sums and other operations have their according counterparts in tensor calculus. As a further step of formal simplification we will assume that all neurons $n_i$ share the same activation function $a$. This does not limit the ability of networks that can be written down, since if two neurons have different activation functions, they can be viewed as two different layers connected to the same previous layer. Their output will than be merged afterwards (see \autoref{fig:diff_activation_functions_layer}).\\
\begin{figure}
\centering
\input{tikzgraphics/tikz_layer_different_activations}
\caption{Depiction of how a layer ($\mathcal{L}_\text{mid}$) consisting of neurons with different activation functions ($n_1$ and $n_2$) can be split into two separate layers ($\mathcal{L}_{n_1}$ and $\mathcal{L}_{n_2}$).}\label{fig:diff_activation_functions_layer}
\end{figure}

\noindent With this simplification one can write a layer simply as
\begin{equation}
\mathcal{L}(W,\vec{b},\vec{x})=a(W\cdot\vec{x}+\vec{b}),
\end{equation}
where it is understood, that the activation function $a$ acts component wise on the resulting $l$-dimensional vector.\\
In this fashion a network consisting of a chain of layers $\mathcal{L}_\text{in}$, $\mathcal{L}_\text{mid}$, $\mathcal{L}_\text{out}$ can be written as
\begin{align}
\mathcal{N} & \lr{W^\text{in}, \vec{b}^\text{in}, W^\text{mid}, \vec{b}^\text{mid}, W^\text{out}, \vec{b}^\text{out}, \vec{x}}\nonumber\\
\phantom{\mathcal{N}} & \coloneqq\mathcal{L}_\text{out}\lr{W^\text{out}, \vec{b}^\text{out}, \mathcal{L}_\text{mid}\lr{W^\text{mid}, \vec{b}^\text{mid}, \mathcal{L}_\text{in}\lr{W^\text{in}, \vec{b}^\text{in}, \vec{x}}}}\nonumber\\
\phantom{\mathcal{N}} & = a_\text{out}\lr{\vec{b}^\text{out}+W^\text{out}\cdot a^\text{mid}\lr{\vec{b}^\text{mid}+W_\text{mid}\cdot a_\text{in}\lr{\vec{b}^\text{in}+W_\text{in}\cdot \vec{x}}}}.
\end{align}
Hence a network can be understood as a set of nested functions.\\
An important point with the definitions above is that the layers get their input only from their preceding layers. Especially no loops are allowed, i.e. getting input from some subsequent layer is not permitted. A network of the first kind is called a \emph{feed forward neural network} (\gls{ffn}), as for one input each layer gets invoked only once. There are also other architectures called \emph{recurrent neural networks} (\gls{rnn}), which also allow for loops in the networks and work by propagating the activations in discrete time steps. These kinds of networks are in principle closer to the inner workings of the human brain, but in practice show worst performance and are therefore not used or discussed further in this work. \textcolor{red}{[Citations], maybe also mention that RNNs have shown good performance in time series data (which we are working with) but other studies (paper Frank sent around) have shown that TCN also do the job}\\
A \gls{ffn} in general consists of three different parts called the input-, output- and hidden layer/layers. The role of the input- and output-layers is self explanatory; they are the layers where data is fed into the network or where data is read out. Therefore their shape is determined by the data the network is being fed and the expected return. The hidden-layers on the contrary are called ''hidden'', as their shape and size is not defined by the data. Furthermore the hidden layers do not see the input or labels directly, which means, that the network itself has to ''decide'' on how to use them (page 165 in \cite{deep_learning_book}). \autoref{fig:hidden_layer} shows an example of a simple network with a single hidden layer. In principle there could be any number of hidden layers with different sizes. In this example the input is n-dimensional and the output 2-dimensional. If the input was changed to be (n-1)-dimensional, the same hidden-layer could be used, as its size does not depend on the data or output. Therefore when designing a network architecture, one designs the shape and functionality of the hidden layers. How well it performs is mainly governed by theses layers. \textcolor{red}{[Can I find citation for this last statement?]}\\
A \gls{nn} is called \emph{deep}, if it has multiple hidden layers. \textcolor{red}{[Citation]}
\begin{figure}
\centering
\input{tikzgraphics/tikz_hidden_layer}
\caption{A depiction of a simple network with a single input-, hidden- and output-layer. The input-data is a n-dimensional vector ${(x_1, \dotsc, x_n)}^T$ and the output is a 2-dimensional vector. In this picture it looks like the hidden layer has the same number of neurons as the input layer. This does not necessarily have to be the case. Lines between two neurons indicate, that the output of the left neuron serves as weighted input for the right one.}\label{fig:hidden_layer}
\end{figure}

\subsection{Backpropagation}\label{sec:backpropagation}
\textcolor{red}{The beginning of this section feels very wordy and repetitive. Break it down!}\\
In \autoref{sec:basics_neuron_network} the basics of a \gls{nn} where discussed and the example of a network replicating a binary full-adder (see \autoref{app:Full_adder}) showed the potential of these networks, when the weights and biases are chosen correctly. The example actually proofs that a sufficiently complicated network can - in principle - calculate any function a computer can, as a computer is just a combination of logic gates, especially binary full-adders.\\
The question therefore is how to choose the weights in a network for it to approximate some function optimally. For the binary full-adder the weights and biases were chosen chosen by hand, as the problem the network was trying to solve was rather simple. A more general approach however would be beneficial, as not all problems are this simple. Therefore the goal is to design some network and let it learn/optimize the weights and biases such that the error between the actual function and the estimate of the network is minimal.\\
To do this, some known and labeled data is necessary, in order for the network being able to compare its output to some ground truth and adjust its weights and biases to minimize some error function. This way of optimizing the weights and biases is called \emph{training}. To be a bit more specific, the analyzed data in this work is some time series. The output of this analysis will be some scalar number; the \gls{snr}. Therefore the network receives some data as input, of which the true \gls{snr}-value is known. This true value will be called \emph{label} from here on out. The network will produce some value from this input data and compare it to what \gls{snr} was provided as label. From there it will try to optimize the weights and biases to best fit the function that maps $\text{data}\to\text{\gls{snr}}$. This process of optimizing the weights and biases in the way described below is enabled by a process called backpropagation, as the error propagates from the last to the first layer. The meaning of this will become clearer in the upcoming paragraphs.\\
The data used for training is usually split into three distinct parts, each used for different purposes. The first one is the training set. This set is used directly by the network to train on. It is the only dataset the network uses to directly improve its weights and biases. The second one is the validation set
So far only the abstract term ''error'' was used. This error, in machine learning language, is called the \emph{loss function} and in general is defined by
\begin{equation}\label{def:general_loss}
L:\R^{l\times k}\times\R^{l\times k}\to\R;\ (y_\text{net}, y_\text{label})\mapsto L(y_\text{net}, y_\text{label}),
\end{equation}
where $l$ is the number of training samples used to estimate the error and $k$ is the dimension of the network output.\\
When doing a regressive fit, one of the standard error functions is the \emph{mean squared error} (\gls{mse}), which is the loss function mainly used in this work and that is defined by
\begin{equation}\label{def:loss_mean_squared_error}
L:\R^{l\times k}\times\R^{l\times k}\to\R;\ (y_\text{net}, y_\text{label})\mapsto L(y_\text{net}, y_\text{label})\coloneqq\frac{1}{l}\sum_{i=1}^l{(\vec{y}_{\text{net},i}-\vec{y}_{\text{label},i})}^2.
\end{equation}
A more thorough discussion and justification for using \gls{mse} as loss can be found in section 5.5 and 6.2.1.1 of \cite{deep_learning_book}.\\
To minimize this loss, the weights and biases of the different layers are changed, usually using an algorithm called \emph{gradient decent}. It works by calculating the gradient of some layer with respect to its weights and biases and taking a step in the opposite direction. For notational simplicity we'll denote the weights and biases of a network by $\theta$ and call them collectively parameters. It is understood that $\theta=\lr{W^{1},b^1, W^2, b^2, \cdots}$. Gradient decent is than given by
\begin{equation}\label{def:gradient_decent}
\theta '=\theta - \epsilon\ \nabla_\theta L\lr{y_\text{net}(\theta), y_\text{label}},
\end{equation}
where $\epsilon$ is called learning rate and controls how large of a step is taken on each iteration.\\
This formula assumes, that all samples from the training set are used to calculate the gradient. In practice this would be too computationally costly. Therefore the training set is split into multiple parts, called mini-batches. A step of the gradient decent is than made using only the samples from one mini-batch. This alteration of gradient decent goes by the name of \emph{stochastic gradient decent}. The larger the mini-batch, the more accurate the estimate of the gradient and therefore fewer steps are needed to get to lower values of the loss. Each step however takes longer to calculate. This means one has to balance the benefits and drawbacks of the mini-batch size.\medskip\\
The real work of training a network now lies in calculating the gradient $\nabla_\theta L\lr{y_\text{net}(\theta), y_\text{label}}$, which is a challenge, as $\theta$ usually consists of at least a few hundred thousand weights and biases. The algorithm, that is used to calculate this gradient, is called backpropagation or simply backprop and is mostly a iterative application of the chain rule.\\
For simplicity assume we have a network $\mathcal{N}\lr{\theta, \vec{x}}$ consisting of $n$ consecutive layers $\mathcal{L}^1,\cdots,\mathcal{L}^n$ with weights $W^1,\cdots,W^n$, biases $\vec{b}^1,\cdots,\vec{b}^n$ and activation functions $a_1,\cdots, a_n$. The network will be trained by minimizing the loss given in \eqref{def:loss_mean_squared_error}. Calculating the gradient $\nabla_\theta L\lr{y_\text{net}(\theta), y_\text{label}}$ requires to calculate $\nabla_{W^1}L, \cdots,\nabla_{W^n}L$ and $\nabla_{\vec{b}^1},\cdots,\nabla_{\vec{b}^n}$, where
\begin{equation}\label{def:matrix_gradient}
\nabla_{W^i}L\coloneqq
\begin{pmatrix}
	\partial_{W^i_{11}}L & \cdots & \partial_{W^i_{1l}}L\\
	\vdots & \ddots & \vdots\\
	\partial_{W^i_{k1}}L & \cdots & \partial_{W^i_{kl}}L
\end{pmatrix},
\end{equation}
for $W^i\in \R^{k\times l}$ and
\begin{equation}\label{def:vector_gradient}
\nabla_{\vec{b}^i}L\coloneqq
\begin{pmatrix}
	\partial_{b^i_1}L\\
	\vdots\\
	\partial_{b^i_k}L
\end{pmatrix},
\end{equation}
for $\vec{b}^i\in\R^k$.\\
To calculate $\partial_{W^i_{jk}}L$ and $\partial_{b^i_j}L$, define
\begin{align}\label{def:backprop_z}
z^n & \coloneqq\vec{b}^n+W^n\cdot a_{n-1}\lr{z^{n-1}}\nonumber\\
z^1 & \coloneqq\vec{b}^1+W^1\cdot\vec{x},
\end{align}
such that
\begin{equation}
\mathcal{N}\lr{\theta,\vec{x}}=a_n\lr{z_n}.
\end{equation}
To save another index, we will assume a mini-batch size of 1. For a larger mini-batch size one simply has to average over the individual gradients, as sums and derivatives commute.\\
With this in mind, the loss is given by
\begin{equation}
L\lr{y_\text{net}, y_\text{label}}=L\lr{\mathcal{N}\lr{\theta,\vec{x}}, y_\text{label}}=L\lr{a_n\lr{z_n}, y_\text{label}}=\lr{a_n\lr{z_n}- \vec{y}_\text{label}}^2.
\end{equation}
To start off derive this loss by some scalar that only $z^n$ depends on.
\begin{equation}
\partial_{\theta_j}\lr{a_n\lr{z_n}- \vec{y}_\text{label}}^2=\lr{\partial_{\theta_j}a_n\lr{z_n}}\lr{2\lr{a_n\lr{z_n}- \vec{y}_\text{label}}}
\end{equation}
From there calculate $\partial_{\theta_j}a_n\lr{z_n}$, remembering, that $a_n$ and $z_n$ are both vectors.
\begin{align}\label{def:start_backprop}
\partial_{\theta_j}a_n\lr{z_n} & = \partial_{\theta_j}\sum_i a_n^i\lr{z_{n,1}\lr{\theta_j},\dotsc, z_{n,k}\lr{\theta_j}} \vec{e}_i\nonumber\\
& = \sum_i \sum_{m=1}^k \lr{\partial_{\theta_j}z_{n, m}\lr{\theta_j}}\lr{\partial_{z_{n, m}} a_n^i\lr{z_{n,1}\lr{\theta_j},\dotsc, z_{n,k}\lr{\theta_j}}} \vec{e}_i\nonumber\\
& = \sum_i \lr{\lr{\partial_{\theta_j}z_n}\cdot \lr{\nabla_{z_n} a_n^i}}\vec{e}_i
\end{align}
Since all activation functions $a_n^i$ on a layer are the same, the gradient $\lr{\nabla_{z_n} a_n^i}$ simplifies to $\partial_z \restr{a(z)}{z=z_{n,i}}$. With this one gets
\begin{equation}\label{def:start_backprop_simple}
\partial_{\theta_j}a_n\lr{z_n} = \lr{\partial_{\theta_j} z_n} \odot \partial_z \restr{a_n(z)}{z=z_n},
\end{equation}
where $\odot$ denotes the Hadamard product.
The final step to understanding backpropagation is to evaluate $\partial_{\theta_j}z_n$. For now assume that $\theta_j$ is some weight on a layer that is not the last layer.
\begin{align}\label{def:backprop_recurs_1}
\partial_{\theta_j}z_n & = \partial_{\theta_j}\lr{\vec{b}^n+W^n\cdot a_{n-1}\lr{z_{n-1}}}\nonumber\\
& = \partial_{\theta_j}W^n\cdot a_{n-1}\lr{z_{n-1}}\nonumber\\
& = W^n\cdot\partial_{\theta_j}a_{n-1}\lr{z_{n-1}}
\end{align}
Inserting \eqref{def:backprop_recurs_1} into \eqref{def:start_backprop_simple} yields the recursive relation
\begin{equation}\label{def:backprop_recursive_relation}
\partial_{\theta_j}a_n\lr{z_n} = W^n\cdot\partial_{\theta_j}a_{n-1}\lr{z_{n-1}}\odot\partial_z \restr{a_n(z)}{z=z_n}.
\end{equation}
The recursion stops, when the layer the recursion is at, is the layer the weight $\theta_j$ is located on. Assuming, the layer weight $\theta_j$ is located on is layer $k$, one gets
\begin{equation}\label{def:backprop_layer_k}
\partial_{\theta_j}a_n\lr{z_n} = \lr{\prod_{l=0}^{n-k}W^{n-l}}\cdot\lr{\partial_{\theta_j}W^k}a_{k-1}\lr{z_{k-1}}\odot\lr{\bigodot_{l=0}^{n-k}\partial_z \restr{a_l(z)}{z=z_l}}.
\end{equation}

\subsection{Specific layers}
\textcolor{blue}{Explain that there are not just dense layers.}
\subsubsection{Dense layer}
\textcolor{blue}{What is a dense layer, how does it work (can maybe be omitted)}
\subsubsection{Convolution layer}
\textcolor{blue}{What are the advantages of convolution layers and why do we use them? Disadvantages?}
\subsubsection{Inception layer}
\textcolor{blue}{Explain what it is, how it works. (cite google paper) ONLY IF IT IS REALLY USED IN THE FINAL ARCHITECTURE!}
\subsubsection{Batch Normalization layer}
\textcolor{blue}{Explain how batch normalization works and why it is useful. (cite according paper)}
\subsubsection{Dropout layer}
\textcolor{blue}{Explain what a dropout layer is, what it does, why it is useful.}
\subsubsection{Max Pooling layer}
\textcolor{blue}{Explain what max pooling does and why it is useful, even when it is counter intuitive.}