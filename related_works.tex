\section{Searching for Gravitational Waves using Neural Networks}\label{sec:related_works}
%\textcolor{blue}{List papers and their findings, that belong in the same area and/or had influence on this work.}\\
%\textcolor{red}{Put motivation here}\\
%\textcolor{blue}{Motivate why the problem we are looking at is interesting and why it might be beneficial to train for SNR instead of simply classifying. Why is BNS more difficult? (Care, I think I wrote a little bit about this in the Data Generating Process section) Maybe drop this if the motivation is clear from the introduction and related works section.}\\
Detecting \gls{gw}s from noisy detector data is a difficult problem as the potential signals are very faint. Current matched filtering based pipelines, as outlined in \autoref{sec:matched_filtering}, are very sensitive but also have two major drawbacks. First of all, they are only proven to be optimal for Gaussian noise. The detector data on the other hand contains non-Gaussian noise transients and thus is not entirely Gaussian or stationary \cite{glitches_gw150914}. For this reason further measures than just using matched filtering have to be taken. Secondly, even though current detection pipelines are able to keep up with the constant data-stream from the detectors, they introduce latency on the order $\mathcal{O}($\SI{10}{\s}$)$. This means that astronomers are alerted of an event about \SI{10}{\s} after the \gls{gw} has passed the detector \cite{pycbc_live}. If the signal came from a binary system containing at least one neutron star an \gls{em} counterpart might be detectable. This counterpart contains a lot of information about the neutron star itself and the evolution of the system \cite{multi_messanger}. Maximizing the observation time is thus very important. Furthermore, the computational cost for matched filter searches grows as detectors get more sensitive since more templates will be needed to cover the entire parameter space. This will either lead to even higher latency or necessitate more powerful hardware.\\
To reduce the latency and computational cost more efficient searches are required. Very promising contenders are neural networks which are known for their computational efficiency once trained. They are furthermore capable of generalizing well to unseen data. As such they might be able to exceed sensitivities of matched filtering based approaches on non-Gaussian noise.\\
For these reasons using \gls{nn}s to analyze data for \gls{gw}s has grown in popularity and multiple very promising results have been found. This section therefore gives an overview of the state of the art machine learning techniques that aim to classify \gls{gw} strain data and puts the thesis at hand into context of these works.\\
The use of deep neural networks as a filter to detect \gls{gw}-signals in noisy detector data was pioneered by Daniel George and E.A. Huerta \cite{original_deep_filtering}. They used a \gls{cnn} to classify time series data into the two categories ''noise + signal'' and ''pure noise'' as well as estimate some source parameters for \gls{bbh} signals. The network was able to reproduce the sensitivity a classical matched filter search could achieve and even showed potential to adapt to eccentric signals which were not used during the training stage. They did, however, calculate their sensitivity at a false alarm probability of 0.6\%, which equates to a false alarm rate of \SI[per-mode=fraction]{155000}{\samples\per\month}. The followup paper \cite{huerta_parameter_estimation} used real detector noise, demonstrating that \gls{cnn}s can be used for real time classification and parameter estimation. The algorithm was furthermore accurate even in the presence of non-Gaussian transients.\\
A similar concept has been applied to the search for continuous \gls{gw}s in \cite{paper_christoph}. The authors show that for a relatively short observation time the performance of their network on low frequency data rivals that of matched filtering, while for higher frequencies or longer observation times their approach falls off. The key advantage of this search is the computational efficiency. Using \gls{cnn}s reduces the pure search time by several orders of magnitude. Even including the time spent during training and for a necessary followup search seems to reduce the total computational cost. However the authors do not get more specific in that regard.\\
The recently published paper \cite{bns_network} tries to classify time series strain data into the three categories ''pure noise'', ''\gls{bbh} signal'' and ''\gls{bns} signal''. As such it is the first publication that tries to classify \gls{bns} signals using a deep \gls{nn}. In that regard it is very similar to our work. This enables us to compare our results to their findings.\\
Both networks are able to consistently recover \gls{bns} signals with a low false positive probability. We do, however, suggest to use not the false positive probability but rather a false alarm rate. Doing so produces results that are more easily comparable to realistic searches. We therefore estimate the false alarm rate at which the authors of \cite{bns_network} generated their results. To do so we use the fact that the \gls{gw}s are shifted by \SI{\pm 0.1}{\s} within the data. Therefore, the network would need to be slid across the data using a step size of \SI{0.1}{\s}. Their testing set contained only 5000 samples, of which $\sim 1667$ were pure noise samples. With so few samples the false alarm rate can only be estimated very roughly and down to \SI[per-mode=fraction]{\sim 15500}{\samples\per\month} (for details on how to do so see \autoref{sec:false_alarm_rate}).\\
Furthermore, the results in \cite{bns_network} are solely given in terms of peak signal-to-noise ratio (\gls{psnr}). The authors quote a conversion factor of $\sim 13$ between the \gls{snr} matched filtering would return and pSNR. The source they used calculated this factor only for \gls{bbh} signals. We found that for \gls{bns} signals this factor is roughly\footnote{We report these values based on the following calculation: We define the peak \gls{snr}, following \cite{bns_network}, as $\max\lr{\text{waveform}} / \sqrt{\text{Var}\lr{\text{noise}}}$. The matched filter \gls{snr} is calculated as described in \autoref{sec:data_generating_process} using a single detector. We than divide the matched filter \gls{snr} by the peak \gls{snr} to get the conversion factor. We use a sample rate of \SI{4096}{\hertz} to generate both noise and signal. Further tests show that this is only beneficial for the claims of \cite{bns_network} as the \gls{psd} increases strongly for large frequencies, thus increasing the conversion factor. At a sample rate of \SI{16}{\kilo\hertz} we found a conversion factor of $\sim 75$.} $39.225$, i.e. $\text{\gls{snr}}\approx 39.225 \text{ pSNR}$.\\
Finally, to compare our results to the findings of \cite{bns_network} we note that our algorithm uses the data of 2 detectors. Therefore, signals that are injected at \gls{snr} $x$ in \cite{bns_network} would on average be assigned a label value of \gls{snr} $\sim \sqrt{2}x$ for our analysis.\\
A detailed comparison can be found in \autoref{sec:network_performance}.\smallskip\\
\begin{comment}
The recently published paper \cite{bns_network} claims to be able to differentiate between the three classes ''noise'', ''\gls{bbh}-signal'' and ''\gls{bns}-signal'' by using a whitened time series of \SI{2}{\s} duration. They are therefore pursuing the same goal as our work. We question their results in multiple aspects based on out own research. First of all, their validation and testing set contain only 5000 samples, with a split of only $1/3$ being pure noise samples. From so few samples it is difficult to get a good estimate of the false alarm rate, due to the lack of noise realizations. This is one of the possibly many factors that lead to their quoted 0\% false alarm probability for \gls{bns} and 1\% for \gls{bbh} signals. Furthermore, they shift their signals around the data by \SI{\pm 0.1}{\s}. With $1667$ noise samples this leads to a false alarm rate of \SI[per-mode=fraction]{\sim 15500}{\samples\per\month} for their loudest noise instance\footnote{We calculated this number as explained below equation \eqref{def:false_alarm_rate}}. Secondly, they only calculate their results in terms of peak signal-to-noise ratio and quote a matched filter signal-to-noise ratio of 13 times the peak signal-to-noise ratio. However, using our own data, we found that a matched filter \gls{snr} of $\sim 15$ corresponds to a peak \gls{snr} of $0.38$. Testing other \gls{snr} values we found a consistent conversion factor of $\text{\gls{snr}}_\text{MF} = 39.225\cdot\text{\gls{snr}}_\text{peak}$\footnote{We report these values based on the following calculation: We define the peak \gls{snr}, following \cite{bns_network}, as $\max\lr{\text{waveform}} / \sqrt{\text{Var}\lr{\text{noise}}}$. The matched filter \gls{snr} is calculated as described in \autoref{sec:data_generating_process} using a single detector. We than divide the matched filter \gls{snr} by the peak \gls{snr} to get the conversion factor. We use a sample rate of \SI{4096}{\hertz} to generate both noise and signal. Further tests show, that this is only beneficial for the claims of \cite{bns_network} as the \gls{psd} increases strongly for large frequencies, thus increasing the conversion factor. At a sample rate of \SI{16}{\kilo\hertz} we found a conversion factor of $\sim 75$.}. Using this conversion factor for peak \gls{snr} to matched filter \gls{snr}, their network has a sensitivity of about 70\% for \gls{bns} signals at matched filter \gls{snr} 15 and unknown false alarm rate. Considering the results of this thesis and the architecture they used, these results sound consistent to our findings. (compare the start of \autoref{sec:evolution_of_architecture}) Finally, their work uses only a single detector. Assuming that both detectors see a signal equally strong means that results they find at \gls{snr} $x$ correspond to signals we find at \gls{snr} $\sim \sqrt{2}x$.\smallskip\\
\end{comment}
\cite{cnn_magiacal_bullet} aims to reduce the confusion that occurs when mixing terms of computer science with those of gravitational data analysis. It criticizes the way statistical significance is claimed by different machine learning approaches. It especially claims that no statistically meaningful false alarm rate can be derived from \gls{nn}s using only training, validation and testing set, if these sets contained individual samples of pure noise and signals. They base their criticism on the fact that a sliding window approach will not always contain the signals in the way the training set suggests. The waveform will not be positioned in just the way it was positioned during the training stage but at some uncontrollable point. Due to this limitation they suggest that \gls{nn}s can only be used as quick and reliable trigger generators that need a followup matched filter search. We try to address most of their points by generating all results and statistics empirically from a long continuous time series. Further measures and precautions will be explained in \autoref{sec:network_performance}.\\
A major contribution relating our final architecture came from \cite{tcn_idea}, the authors of which compared the performance of multiple different general architectures in the field of gravitational wave data analysis. Their findings indicated that a \gls{tcn} was the best algorithm for this task. They were also able to compare current feed forward neural networks against recurrent neural networks in the scope of \gls{gw} data analysis and concluded that they in some cases outperform \gls{cnn} architectures but can't match their \gls{tcn}.\\
\cite{dnn_denoising} takes a similar approach to their network architecture as \cite{tcn_idea} does but uses it to denoise the input data, i.e. recover the waveform from the noisy detector data. They show that the network is able to recover \gls{bbh} signals to incredible accuracy and thus can learn the characteristics of the noise background. We use their idea as part of our final network.
\newpage
$\ $
\newpage