\section{Tested networks}
\textcolor{blue}{The title needs to be improved!\\Explain what we are trying to do once more and how our net achieves that.}
\subsection{The data generating process}
\textcolor{blue}{Explain how the data is generated (especially that it only contains gaussian noise). Explain how training/validation set are different from the testing set used in this work. Explain how we carefully chose the parameters, especially how we got the SNR-values. Also explain the mix-and-match approach, that noise can be pure or the same noise with different signals. Also the same signal can (but this isn't necessary) have multiple noise instances. The pure noise SNR was set to 4, as that is the average value for pure noise matched filtering returns. (One might want to alter this) Finally we were careful, that no noise instance or signal from the training set is used in the validation set.}\\
To train a \gls{nn} to detect and classify \gls{bns} signals, we need a large set of mock data. \gls{bns} signals are more difficult to train for than \gls{bbh} signals, as their signals are a lot weaker and span a last for a longer duration\footnote{\gls{bbh} signals span about \SI{1}{\s}, whereas \gls{bns} signals can be visible in the whitened detector data for multiple tens of seconds \cite{gw170817} and when generated even last multiple hundred seconds.}. Training a \gls{nn} on such large data, that is sampled sufficiently to resolve the highest frequencies occurring during the merger \textcolor{red}{(Need to introduce Inspiral/Merger/Ringdown in 2.1)}, is not feasible. As we want to loose as little \gls{snr} as possible, we also do not want to crop the data to contain only the merger and very little of the inspiral. For these reasons we introduce a new multi-rate approach. The network does not receive the data at a fixed sample-rate, but rather multiple inputs, each sampling parts of the signal at different rates.\\
We choose the window to encompass \SI{64}{\s} of data, where a signal is aligned such, that the merger is roughly \SI{0.5}{\s} from the end of the data. Afterwards we chop the data into parts of duration \SI[parse-numbers=false]{2^i}{\s} and re-sample each of these parts to have a sample rate of \SI[parse-numbers=false]{2^{12-i}}{\hertz}, with $i=0,\dotsc 6$.
\subsection{Different non-optimal approaches}
\textcolor{blue}{Talk about the different things we tried, what didn't work and how we improved on them. (Improvement from convolution to inception, improvement from inception to collect-inception, improvement from inception to tcn-inception, improvement of tcn-collect-inception over collect-inception.)}
\subsection{Final network}
\textcolor{blue}{Talk about how the final network looks, how it performs and what could be imporved.}
\subsubsection{Architecture}
\textcolor{blue}{Explain the architecture and the reasoning behind it. Talk about the size of the model, where it could be trained and what the drawbacks of the architecture are. (Drawbacks: Large memory size (hence small batch size), very deep $\to$ slow training and maybe vanishing gradients, hyper-parameter-optimization is hard, not everywhere are residual connections (fixable by further dimensional reduction))}
\subsubsection{Network performance}
\textcolor{blue}{Evaluate the performance of the network. Show sensitivity curves, talk about speed advantages, how does it in both cases compare to matched filtering? How does it compare to related works? (Reference the BNS-Net paper, what is different between our approach and theirs? Why does theirs seem to work a lot better? Does it?)}
%\textcolor{blue}{Compare the speeds of both methods, the accuracy. What kind of drawbacks does the network have, what are its advantages, how should it be used and understood? (Not as a standalone method of analysis but rather as a starting point for samplers and a quick way of estimate certain properties.)}
