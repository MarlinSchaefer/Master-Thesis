\section{Tested networks}
\textcolor{blue}{The title needs to be improved!\\Explain what we are trying to do once more and how our net achieves that.}
\subsection{The data generating process}
\textcolor{blue}{Explain how the data is generated (especially that it only contains gaussian noise). Explain how training/validation set are different from the testing set used in this work. Explain how we carefully chose the parameters, especially how we got the SNR-values. Also explain the mix-and-match approach, that noise can be pure or the same noise with different signals. Also the same signal can (but this isn't necessary) have multiple noise instances. The pure noise SNR was set to 4, as that is the average value for pure noise matched filtering returns. (One might want to alter this) Finally we were careful, that no noise instance or signal from the training set is used in the validation set.}\\
To train a \gls{nn} to detect and classify \gls{bns} signals, we need a large set of mock data. \gls{bns} signals are more difficult to train for than \gls{bbh} signals, as their signals are a lot weaker and last for a longer duration\footnote{\gls{bbh} signals spend about \SI{1}{\s} within the sensitive frequency range of our detectors, whereas \gls{bns} signals can be visible in the whitened detector data for multiple tens of seconds \cite{gw170817} and when generated even last multiple hundred seconds.}. Training a \gls{nn} on such large data, that is sampled sufficiently to resolve the highest frequencies occurring during the merger \textcolor{red}{(Need to introduce Inspiral/Merger/Ringdown in 2.1)}, is not feasible. As we want to loose as little \gls{snr} as possible, we also do not want to crop the data to contain only the merger and very little of the inspiral.\\
Although \gls{bns} signals spend a long time in sensitive region of the detectors, their frequency evolution for large parts of the signal is rather slow, as a lot more of the inspiral is visible. The inspiral having larger frequencies is due to the lower masses involved. (Compare \eqref{def:frequency_evolution_differential} and \eqref{def:frequency evolution}) For the inspiral-part of the signal, frequencies are around \SIrange{30}{100}{\hertz}. To resolve these frequencies, a sample rate of \SIrange{60}{200}{\hertz} is necessary. The higher sample rates are only required for the final few seconds. For these reasons we introduce a new multi-rate approach. The network does not receive the data at a fixed sample-rate, but rather multiple inputs, each sampling parts of the signal at different rates.\\
We choose the window to encompass \SI{64}{\s} of data, where a signal is aligned such, that the merger is roughly \SI{0.5}{\s} from the end of the data. Afterwards we chop the data into parts of duration \SI[parse-numbers=false]{2^i}{\s} and re-sample each of these parts to have a sample rate of \SI[parse-numbers=false]{2^{12-i}}{\hertz}, with $i=0,\dotsc ,6$. This way each sample rate contains exactly $4096$ samples.\\
Each sample rate however overlaps with the higher sample rates, e.g. the \SI{2}{\s} interval sampled at \SI{2048}{\hertz} also includes the \SI{1}{\s} interval sampled at \SI{4096}{\hertz}. For this reason and to reduce the number of input samples even further, we only use the first $2048$ samples for each rate, except the highest sample rate. To keep things simple however, the highest sample rate is split into two parts, each containing $2048$ samples. Therefore each \SI{64}{\s} input interval is split and re-sampled to yield 8 inputs, each containing $2048$ samples. \textcolor{red}{(Include graphic visualizing multi-rate-filtering)}
\medskip\\
To obtain a large set of training data, fake data is generated by utilizing the PyCBC software package \cite{pycbc}. \textcolor{red}{Check which version I'm using and cite that one.} The final training set contained $56250$ different signals and $161250$ different noise realizations. All waveforms were generated using the approximant ''TaylorF2'', as implemented by PyCBC. \textcolor{red}{(Should this be Lal?)} Out of the 17 parameters that could have been varied, we chose to fix the spins to be $0$ and neglected tidal effects for simplicity. Furthermore the coalescence time $t_\text{coal}$ is set to be $0$. The remaining $8$ parameters were chosen to represent a realistic distribution, in order to estimate the potential of our approach in a real search. As such, both component masses $m_1$ and $m_2$ are uniformly distributed in the range \SIrange{1.2}{1.6}{M_\odot} each. Specifically we do not explicitly require $m_1\geq m_2$ when generating the waveform. The coalescence phase $\Phi_0$  and the polarization angle $\psi$ are uniformly distributed on the interval $\left[0, 2\pi\right]$ and the inclination $\iota$ is distributed like $\arccos\lr{\text{uniform}\lr{-1, 1}}$. Finally the sky-position is isotropic, i.e. $\theta$ is distributed like $\arccos\lr{\text{uniform}\lr{-1, 1}}$ and $\varphi$ uniform in $\left[-\pi, \pi\right]$.\\
The luminsoity distance $r$ is not chosen directly, but indirectly by fixing the \gls{snr} to some value. This is valid, as the \gls{snr} scales inversely with the distance. \textcolor{red}{(Is this true, or is for instance $SNR\approx 1/r^2$?))} In this work, the \gls{snr} is uniformly distributed on the interval $\left[8,15\right]$. One has to avoid one major pitfall when fixing or calculating the \gls{snr} and comparing it to other results. If one compares the \gls{snr} of two signals with the same parameters, the value will heavily depend on the length of the segment used. According to \textcolor{red}{Reference to matched filtering section}, cutting off the waveform early might result in a lower or at least inaccurate value of the \gls{snr}. For this reason, to make meaningful statements about the \gls{snr} of a signal, we need to specify how we calculated it. For this work, we generate the waveforms with a lower frequency cutoff of \SI{20}{\hertz}, which results in waveforms, with a duration of about \SI{500}{\s}. After these waveforms are generated, we calculate the projection onto the detectors and crop the signals in such a way, that they span \SI{96}{\s} and the merger lies within the last second. Only after the waveforms are cropped, we calculate the \gls{snr} with no noise present (while assuming the \gls{psd} of the detector) using the waveform itself. Since we are using multiple detectors, this value $\rho_i$ is calculated for each detector. The total \gls{snr} in the absence of noise is given by
\begin{equation}
\rho_\text{total} = \sqrt{\sum_i\rho_i^2}.
\end{equation}
Each waveform is than rescaled by multiplying with the factor $\text{\gls{snr}} / \rho_\text{total}$. As a last step, before re-sampling the data as described above, the waveforms are whitened. For the training and validation set this whitening procedure is done using the analytic \gls{psd} \verb|aLIGOZeroDetHighPower| provided by PyCBC, instead of using an estimate of the \gls{psd} based on the data itself. The testing set however uses the estimate of the \gls{psd}, and not the analytic form. The reason to treat these two sets differently from the testing set, is the way, the samples are fed to the network. For the training/validation set, we store the noise samples and pure signals separately and only add them as a first step of the network. For this reason, an estimate of the \gls{psd} would be meaningless at least for the pure signals. For the testing set however we want to mimic, how a pipeline of a detector might work. For this scenario using an analytic model would not be accurate. Therefore in this step the \gls{psd} is only estimated.\\
The reason for storing noise and signals separately are resource constraints. To cover the entire parameter-space densely enough and avoid overfitting, a large number of samples are necessary. Initially we generated and stored the sum of signal and noise, instead of storing each category separately. This has multiple disadvantages, but also one key advantage; we can use the pure signal as a filter for the optimal filter \textcolor{red}{(Insert equation number here)} and give a best-case recovery \gls{snr}, that a search could return. In that sense, we could monitor the performance and compare it to matched filtering directly during training. The disadvantages however at some point outweighed this advantage. The core disadvantage was the restricted number of samples. A file containing $500,000$ samples has a file-size on the order of \SI{200}{\giga\byte}. To train the networks on the data, we completely load it into system memory and need some overhead for formatting. To reduce these costs, we decided to split the signal- and noise-samples and only at runtime choose one instance of each category, which are than added together on the first layer of the network. The second advantage of this approach is less obvious. It enables us to easily feed the network the same signal submerged in multiple different noise realizations, which resulted in performance improvements for similar tasks (Christoph Drei√üigacker, personal communication, June 2019).\\
The split between training and validation set is treated with great care, assuring, that not a single noise or signal sample from the training set is used during validation. Therefore the reported loss and accuracy values are representative of a real search. Though they are not the final statistic, we report, they are tightly linked to those and give clues about the network and its efficiency. In particular, the data used for training and validating the final network contained $75,000$ different \gls{gw}-signals and $215,000$ noise realizations. We than generate a set number of unique index pairs $(s_i, n_i)$, where $s_i$ corresponds to a signal and $n_i$ to a noise sample. For the training set these indices may be selected from $s_i\in\left[0, 3/4s_t\right]$ and $n_i\in\left[0, 3/4n_t\right]$, where $s_t$ and $n_t$ are the total number of signals and noise samples respectively. If $3/4s_t\notin\mathcal{N}$, the upper index is rounded to the nearest natural number. The same is true for $3/4 n_t$.

\subsection{Different non-optimal approaches}
\textcolor{blue}{Talk about the different things we tried, what didn't work and how we improved on them. (Improvement from convolution to inception, improvement from inception to collect-inception, improvement from inception to tcn-inception, improvement of tcn-collect-inception over collect-inception.)}
\subsection{Final network}
\textcolor{blue}{Talk about how the final network looks, how it performs and what could be imporved.}
\subsubsection{Architecture}
\textcolor{blue}{Explain the architecture and the reasoning behind it. Talk about the size of the model, where it could be trained and what the drawbacks of the architecture are. (Drawbacks: Large memory size (hence small batch size), very deep $\to$ slow training and maybe vanishing gradients, hyper-parameter-optimization is hard, not everywhere are residual connections (fixable by further dimensional reduction))}
\subsubsection{Network performance}
\textcolor{blue}{Evaluate the performance of the network. Show sensitivity curves, talk about speed advantages, how does it in both cases compare to matched filtering? How does it compare to related works? (Reference the BNS-Net paper, what is different between our approach and theirs? Why does theirs seem to work a lot better? Does it?)}
%\textcolor{blue}{Compare the speeds of both methods, the accuracy. What kind of drawbacks does the network have, what are its advantages, how should it be used and understood? (Not as a standalone method of analysis but rather as a starting point for samplers and a quick way of estimate certain properties.)}
