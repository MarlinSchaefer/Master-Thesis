\section{Full adder as network}\label{app:Full_adder}
To create a full adder from basic neurons, the corresponding logic gates need to be defined. The equivalent neuron for an ''and''-gate was defined in \autoref{sec:basics_neuron_network}. There are two more basic neurons which need to be defined. The neuron corresponding to the ''or''-gate, which is given by the same activation function \eqref{def:step_activation}, weights $\vec{w}={(w_1, w_2)}^T=(1,1)$ and bias $b=-0.5$, and the equivalent neuron for the ''not''-gate, which is given by the activation function \eqref{def:step_activation}, weight $w=-1$ and bias $b=0.5$. These definitions are summarized in \autoref{tab:neuron_logic_gates}.
\begin{table}[H]
\begin{center}
\begin{tabular}{c|c|c}
''and''-neuron & ''or''-neuron & ''not''-neuron\\
\hline
\input{tikzgraphics/tikz_and_neuron} & \input{tikzgraphics/tikz_or_neuron} & \input{tikzgraphics/tikz_not_neuron}\\
\hline
\begin{tabular}{c c}
$\vec{w}=(1,1)$ & $b=-1.5$
\end{tabular} &
\begin{tabular}{c c}
$\vec{w}=(1,1)$ & $b=-0.5$
\end{tabular} &
\begin{tabular}{c c}
$w=-1$ & $b=0.5$
\end{tabular}\\
\hline
\begin{tabular}{c c|c}
$x_1$ & $x_2$ & $a(x_1+x_2-1.5)$\\
\hline
$0$ & $0$ & $0$\\
$0$ & $1$ & $0$\\
$1$ & $0$ & $0$\\
$1$ & $1$ & $1$\\
\end{tabular} &
\begin{tabular}{c c|c}
$x_1$ & $x_2$ & $a(x_1+x_2-0.5)$\\
\hline
$0$ & $0$ & $0$\\
$0$ & $1$ & $1$\\
$1$ & $0$ & $1$\\
$1$ & $1$ & $1$\\
\end{tabular} &
\begin{tabular}{c|c}
$x_1$ & $a(-x_1+0.5)$\\
\hline
$0$ & $1$\\
$1$ & $0$\\
\end{tabular}
\end{tabular}
\end{center}
\caption{A summary and depiction of the main logic gates written as neurons. All of them share the same activation function \eqref{def:step_activation}.}\label{tab:neuron_logic_gates}
\end{table}
\medskip
\noindent Using the basic logic gates a more complex structure - the ''XOR''-gate - can be built. A ''XOR''-gate is defined by its truth table (see \autoref{tab:xor}).
\begin{table}[H]
\centering
\begin{tabular}{c c|c}
$x_1$ & $x_2$ & $x_1\veebar x_2$\\
\hline
$0$ & $0$ & $0$\\
$0$ & $1$ & $1$\\
$1$ & $0$ & $1$\\
$1$ & $1$ & $0$
\end{tabular}
\caption{Truth table for the ''XOR''-gate.}\label{tab:xor}
\end{table}
It can be constructed from the three basic logic operations ''and'', ''or'' and ''not''
\begin{equation}
x_1\veebar x_2 = \lnot\lr{\lr{x_1\land x_2}\lor\lnot\lr{x_1\lor x_2}}.
\end{equation}
Therefore the basic neurons from \autoref{tab:neuron_logic_gates} can be combined to create a ''XOR''-network (see \autoref{fig:xor_net}).\\
To simplify readability from here on out a neuron called ''XOR'' will be used. It is defined by the network of \autoref{fig:xor_net} and has to be replaced by it, whenever it is used.\\
With this ''XOR''-neuron a network, that behaves like a full-adder, can be defined. A full-adder is a binary adder with carry in and carry out, as seen in \autoref{fig:full_adder}.
\begin{figure}[H]
\centering
\input{tikzgraphics/tikz_xor_net}
\caption{The definition of a network that is equivalent to an ''XOR''-gate.}\label{fig:xor_net}
\end{figure}
\begin{figure}[H]
\centering
\input{tikzgraphics/tikz_full_adder}
\caption{A network replicating the behavior of a binary full adder.}\label{fig:full_adder}
\end{figure}

\section{Indication that the network does not learn}\label{app:network_does_not_learn}
Sometimes during training only a local minimum is found. The most notable of these is when the network just picks a fixed value in the interval and appoints it to any input. It is therefore useful to spot this behavior during training and consider restarting the training again. Therefore the value for a constant output will be calculated in this appendix.
\medskip\\
To start off the loss function will be assumed to be the mean squared error and that the \gls{snr}-values lie within the interval $\left[a,b\right]$. The mean squared error of a chosen point $x$ to every point in this interval is given by
\begin{equation}
f(x)\coloneqq\frac{1}{b-a}\int_a^b\diff y\ \lr{x-y}^2=\frac{1}{3\lr{b-a}}\lr{\lr{b-x}^3-\lr{a-x}^3}.
\end{equation}
To minimize this one could in principle solve $\frac{\partial f}{\partial x}\mbe 0$ and it would yield the correct result. However it should at least intuitively be obvious that the point that minimizes the mean squared error to every point in the interval is the mean value of the interval $x=\frac{a+b}{2}$.\\
Therefore if the network has to choose values out of a given interval $\left[a,b\right]$, minimize the mean squared error and arrives in a local minimum that leads to the network always returning a fixed value, this value should be
\begin{equation}
f\lr{\frac{a+b}{2}}=\frac{1}{3\lr{b-a}}\lr{\lr{\frac{b-a}{2}}^3-\lr{\frac{a-b}{2}}^3}=\frac{1}{12}\lr{b-a}^2.
\end{equation}
In this work another common case is the network having to choose a \gls{snr}-value from some continues interval $\left[a,b\right]$ or pick a discrete value $c$. The interval corresponds to the data containing some \gls{gw}-signal and the fixed point with value $c$ is the \gls{snr}-value assigned to pure noise during training. (This is by no means a strictly mathematical derivation but simply a quick way to calculate the expected value.) The mean squared error is hence given by
\begin{align}
g(x) & \coloneqq\lim_{n\to\infty}\lr{\frac{1}{n}\sum_{i=1}^{n}\lr{x-y_i}^2}\text{ with }
\begin{cases}
	y_i=a_i\in\left[a,b\right]\text{, probability }p\\
	y_i=c\text{, probability }(1-p)
\end{cases}\nonumber\\
& = \lim_{n\to\infty}\lr{\frac{1}{n}\sum_{i=1}^{p\cdot n}\lr{x-a_i}^2}+\lim_{n\to\infty}\lr{\frac{1}{n}\sum_{i=1}^{(1-p)\cdot n}\lr{x-c}^2}\nonumber\\
& = \lim_{n\to\infty}\lr{\frac{1}{n}\sum_{i=1}^{p\cdot n}\lr{x-a_i}^2}+\lim_{n\to\infty}\lr{\frac{(1-p)\cdot n}{n}\lr{x-c}^2}\nonumber\\
& \overset{(\ast)}{=} \lim_{n\to\infty}\lr{\frac{p}{n}\sum_{i=1}^{n}\lr{x-a_i}^2}+(1-p)\lr{x-c}^2\nonumber\\
& = p\underbrace{\lim_{n\to\infty}\lr{\frac{1}{n}\sum_{i=1}^{n}\lr{x-a_i}^2}}_{=f(x)}+(1-p)\lr{x-c}^2\nonumber\\
& = p f(x) + (1-p)\lr{x-c}^2,
\end{align}
where the step in $(\ast)$ is not clear and would need a mathematical proof, but intuitively should be clear. \textcolor{red}{(If there is time, find a proof.)} For large $n$ all $a_i$ should contribute equally to the mean value and hence $p$ is just a proportionality factor.\\
With $\partial_x f(x)=2x-a-b$ one gets
\begin{equation}
\partial_x g(x) \mbe 0 \Leftrightarrow x = \frac{p}{2}\lr{a+b}+\lr{1-p}c
\end{equation}
as expected. The value of $g$ at this point will be the expectation value of the mean squared error if the network predicts a single value and optimizes this value. In this work $p$ is the probability of looking at data containing a \gls{gw}, i.e. the fraction of data-points containing a \gls{gw} over total number of data-points.